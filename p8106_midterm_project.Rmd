---
title: "P8106_midterm_project_v1"
author: "Congyu Yang, Yujing Fu, Zhengkun Ou"
date: "2025-03-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)
p_load(tidyverse, caret, tidymodels, corrplot, ggplot2, plotmo, ggrepel, patchwork, earth, pdp, mgcv, knitr)

```

```{r}
load("dat1.RData")
load("dat2.RData")
```

```{r}
str(dat1)
```

```{r}
skimr::skim(dat1)
skimr::skim(dat2)
```
There is no missing value in this dataset. 

```{r}
colnames(dat1)
```

```{r}
dat1 = dat1 |> janitor::clean_names()
dat2 = dat2 |> janitor::clean_names()
```
We can see there is no NA values in the dataset. 

## Convert to the factor variable
```{r}
dat1$gender <- factor(dat1$gender, levels = c(0, 1), labels = c("Female", "Male"))
dat1$race <- factor(dat1$race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic"))
dat1$smoking <- factor(dat1$smoking, levels = c(0, 1, 2), labels = c("Never", "Former", "Current"))
dat1$diabetes <- factor(dat1$diabetes, levels = c(0, 1), labels = c("No", "Yes"))
dat1$hypertension <- factor(dat1$hypertension, levels = c(0, 1), labels = c("No", "Yes"))

dat2$gender <- factor(dat2$gender, levels = c(0, 1), labels = c("Female", "Male"))
dat2$race <- factor(dat2$race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic"))
dat2$smoking <- factor(dat2$smoking, levels = c(0, 1, 2), labels = c("Never", "Former", "Current"))
dat2$diabetes <- factor(dat2$diabetes, levels = c(0, 1), labels = c("No", "Yes"))
dat2$hypertension <- factor(dat2$hypertension, levels = c(0, 1), labels = c("No", "Yes"))

```

```{r}
str(dat1)
```
```{r}
summary_stats <- summary(dat1)
print(summary_stats)
```
```{r}
# Check for missing values
missing_values <- colSums(is.na(dat1))
print(missing_values)
```
There is no missing value in the dataset. 

Check the distribution of the outcome variable in the dataset. 
```{r}
# Distribution of the response variable
p1 <- ggplot(dat1, aes(x = log_antibody)) + 
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Distribution of Log-Transformed Antibody Levels",
       x = "Log Antibody Level", y = "Frequency") +
  theme_minimal()
p1
```
```{r}
# Boxplot of log_antibody by gender
p2 <- ggplot(dat1, aes(x = gender, y = log_antibody, fill = gender)) +
  geom_boxplot() +
  labs(title = "Antibody Levels by Gender",
       x = "Gender", 
       y = "Log Antibody Level") +
  theme_minimal()
p2
```

```{r}
# Boxplot of log_antibody by race
p3 <- ggplot(dat1, aes(x = race, y = log_antibody, fill = race)) +
  geom_boxplot() +
  labs(title = "Antibody Levels by Race/Ethnicity",
       x = "Race/Ethnicity", 
       y = "Log Antibody Level") +
  theme_minimal()

```

```{r}
p4 <- ggplot(dat1, aes(x = smoking, y = log_antibody, fill = smoking)) +
  geom_boxplot() +
  labs(title = "Antibody Levels by Smoking Status",
       x = "Smoking Status", 
       y = "Log Antibody Level") +
  theme_minimal()

# Boxplot of log_antibody by diabetes status
p5 <- ggplot(dat1, aes(x = diabetes, y = log_antibody, fill = diabetes)) +
  geom_boxplot() +
  labs(title = "Antibody Levels by Diabetes Status",
       x = "Diabetes Status", 
       y = "Log Antibody Level") +
  theme_minimal()
```

```{r}
p2+p3+p4+p5
```
```{r}
# Scatterplot matrix for continuous variables
continuous_vars <- dat1 %>% 
  select(age, height, weight, bmi, sbp, ldl, time, log_antibody)

# Calculate correlation matrix
correlation_matrix <- cor(continuous_vars)
corrplot(correlation_matrix, method = "circle", type = "upper", 
         tl.col = "black", tl.srt = 45)
```


```{r}
# Create scatterplots for continuous variables vs response
p6 <- ggplot(dat1, aes(x = age, y = log_antibody)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red") +
  labs(title = "Age vs Log Antibody Level", 
       x = "Age (years)", y = "Log Antibody Level") +
  theme_minimal()

p7 <- ggplot(dat1, aes(x = bmi, y = log_antibody)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red") +
  labs(title = "BMI vs Log Antibody Level", 
       x = "BMI", y = "Log Antibody Level") +
  theme_minimal()

p8 <- ggplot(dat1, aes(x = time, y = log_antibody)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", color = "red") +
  labs(title = "Time Since Vaccination vs Log Antibody Level", 
       x = "Time (days)", y = "Log Antibody Level") +
  theme_minimal()
```
```{r}
p6+p7+p8
```
There are some potential nonlinear trend between log_antibody and bmi, and between  log_antibody and time.

# 3. MODEL TRAINING WITH CROSS-VALIDATION ----------------------------------------

# Create formula for modeling
```{r}
model_formula <- log_antibody ~ . - id

# Set up cross-validation control
ctrl <- trainControl(method = "cv", number = 10)

# 3.1 Linear Regression
set.seed(123)
lm.fit <- train(
  model_formula,
  data = dat1,
  method = "lm",
  trControl = ctrl
)
print(lm.fit)
```
# 3.2 Ridge Regression
```{r}
set.seed(123)
ridge.fit <- train(
  model_formula,
  data = dat1,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 0,
                         lambda = exp(seq(6, -6, length = 100))),
  trControl = ctrl
)
print(ridge.fit)
print(ridge.fit$bestTune)
plot(ridge.fit, xTrans = log)
```
```{r}
ridge.fit$bestTune
```
# 3.3 Lasso Regression
```{r}
set.seed(123)
lasso.fit <- train(
  model_formula,
  data = dat1, 
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1,
                         lambda = exp(seq(6, -6, length = 100))),
  trControl = ctrl
)
print(lasso.fit)
print(lasso.fit$bestTune)
plot(lasso.fit, xTrans = log)
```

```{r}
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
```
# 3.4 Elastic Net Regression
```{r}
set.seed(123)
enet.fit <- train(
  model_formula,
  data = dat1,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                         lambda = exp(seq(6, -6, length = 100))),
  trControl = ctrl
)
print(enet.fit)
print(enet.fit$bestTune)
```
```{r}
# Plot elastic net results with different colors for each alpha
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(enet.fit, par.settings = myPar, xTrans = log)

# Show coefficients of the final Elastic Net model
coef_enet <- coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
print(coef_enet)
```
```{r}
coef_enet <- coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
print(coef_enet)
```
## Resamples
```{r}
resamp <- resamples(list(
  Linear_Regression = lm.fit,
  Ridge = ridge.fit, 
  Lasso = lasso.fit,
  Elastic_Net = enet.fit
))
summary(resamp)
```
```{r}
parallelplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "RMSE")
```
The RMSE from resampling looks similar to each other. 

```{r}
# Function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}
```

```{r}
# Make predictions using each model
lm.pred <- predict(lm.fit, newdata = dat2)
ridge.pred <- predict(ridge.fit, newdata = dat2)
lasso.pred <- predict(lasso.fit, newdata = dat2)
enet.pred <- predict(enet.fit, newdata = dat2)
```

```{r}
# Calculate test RMSE for each model
lm.test.error <- mean((lm.pred - dat2[,"log_antibody"])^2)
ridge.test.error <- mean((ridge.pred - dat2[,"log_antibody"])^2)
lasso.test.error <- mean((lasso.pred - dat2[,"log_antibody"])^2)
enet.test.error <- mean((enet.pred - dat2[,"log_antibody"])^2)
```

```{r}
test_performance <- data.frame(
  Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", 
            "Elastic Net Regression"),
  Test_ERROR = c(lm.test.error, ridge.test.error, lasso.test.error, 
                enet.test.error)
)
print(test_performance)
```
They are very similar to each other. We probably want to choose the Lasso model as it is the simplest model among those four models. 



### Smoothing Spline
We use scatterplot to explore the relationship between the log antibody level and other variables. Time and bmi tend shows potentially nonlinear trend.
```{r}
x = model.matrix(log_antibody ~ . - id, data = dat1)[, -1]
y = dat1[,"log_antibody"]
x_test = model.matrix(log_antibody ~ . - id, data = dat2)[, -1]
y_test = dat2[,"log_antibody"]
```


```{r}
# choose the best df
fit.ss = smooth.spline(dat1$bmi, dat1$log_antibody)
fit.ss$df
```


```{r}
# plot optimal fit
bmi.grid = seq(from = 17.5, to = 40, by = 1)

pred.ss = predict(fit.ss, x = bmi.grid)
pred.ss.df = data.frame(pred = pred.ss$y, bmi = bmi.grid)


p = ggplot(dat1, aes(x = bmi, y = log_antibody)) +
  geom_point(color = rgb(.2, .4, .2, .5), size = 1) 
  theme_bw()

p +
geom_line(aes(x = bmi, y = pred), data = pred.ss.df,
color = rgb(.8, .1, .1, 1)) + theme_bw()

```

```{r}
# mse
pred_ss = predict(fit.ss, x = dat2$bmi)

mse_ss = mean((dat2$log_antibody - pred_ss$y)^2)
mse_ss
```


### MARS
```{r}
set.seed(2)
ctrl1 = trainControl(method = "cv", number = 10)
mars_grid = expand.grid(degree = 1:3, nprune = 2:20)
mars.fit = train(x, y, method = "earth", tuneGrid = mars_grid,
trControl = ctrl1)
ggplot(mars.fit)

```

```{r}
mars.fit$bestTune
```

```{r}
coef(mars.fit$finalModel)
```

```{r}
summary(mars.fit$finalModel)
```

```{r}
# we choose the relatively important variables to draw partial dependence plot
pdp1 = pdp::partial(mars.fit, pred.var = c("bmi"), grid.resolution = 10) |> autoplot()
pdp2 <- pdp::partial(mars.fit, pred.var = c("bmi", "time"), grid.resolution = 10) |>
pdp::plotPartial(levelplot = FALSE, zlab = "log antibody", drape = TRUE,
screen = list(z = 20, x = -60))
gridExtra::grid.arrange(pdp1, pdp2, ncol = 2)

```

```{r}
# test error
pred_mars = predict(mars.fit, newdata = x_test)

mse_mars = mean((y_test - pred_mars)^2)
mse_mars
```
Test MSE is `r mse_mars`.


### GAM

```{r}
set.seed(2)
gam.fit = train(x, y, method = "gam", trControl = ctrl1)

summary(gam.fit)
```

```{r}
gam.fit$bestTune
```

```{r}
gam.fit$finalModel
```


```{r}
pred_gam = predict(gam.fit, newdata = x_test)
mse_gam = mean((y_test - pred_gam)^2)
mse_gam
```

Test MSE for GAM model is `r mse_gam` 

## Comparison between linear and non-linear model using test error
```{r}
comparison = resamples(list(MARS = mars.fit, GAM = gam.fit))
summary(comparison)
```


```{r}
# MSE comparison

test_mse_table = data.frame(
  Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", 
            "Elastic Net Regression","Smoothing Spline", "MARS", "GAM"),
  MSE = c(lm.test.error,ridge.test.error,
          lasso.test.error, mse_ss, enet.test.error, 
          mse_mars, mse_gam)
) |> arrange(MSE)

kable(test_mse_table, sort = TRUE)

```


The best prediction model is ....

